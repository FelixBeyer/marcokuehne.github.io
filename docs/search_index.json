[["index.html", "Becoming Fluent in Data A Personal Journey – Every Time. Dust and Dark", " Becoming Fluent in Data A Personal Journey – Every Time. Marco Kühne 2022-09-06 Dust and Dark A dusty lecture hall. The light cut through the darkness from the left side of the room. A dozen of seats in each bench, only few occupied by small groups of students who were trying to make sure that they sit far from each other and as far as possible from the lecturer. The bearish but competent assistant professor explained how to analyze and evaluate the results of various memory and cognition experiments through boxplots, t-test and the like in that software. My creaky, slow but loyal laptop in front of me. That’s where R was introduced in my psychology undergraduate studies. – The Times They Are A-Changin’. Lecture Hall. Melanchthonianum. MLU University of Halle-Wittenberg "],["teach-learn-repeat.html", "Teach – Learn – Repeat", " Teach – Learn – Repeat Teaching and learning are strongly connected. I fell in love with learning by teaching the moment I came across this concept. It put the experiences I made into scientific context. Studying for the undergraduate math classes, I soon became head of the study group, than a private tutor, than a student assistant and a doctoral student, now, teaching stuff for over a decade. Still, I feel that (trying) to teach stuff is the best way of learning it myself. By writing the gitbook I hope to force myself to pinpoint exactly what I know and don’t know about data and how to fill the gaps. Luckily, I am not alone with the approach of creating classes or writing books to learn: I could feel that econometrics was indispensable, and yet I was missing something. But what? It was a theory of causality […]. So, desperate, I did what I always do when I want to learn something new — I developed a course on causality to force myself to learn all the things I didn’t know. Scott Cunningham. In: Causal Inference – The Mixtape This project helped me to learn more about R, RStudio, R Markdown, R Bookdown, HTML/CSS, Git and Github, empirical research, causal inference, statistics, math, frustration tolerance and fun. "],["aim-of-this-books.html", "Aim of this books", " Aim of this books I welcome you to join me on the way to become fluent in data. The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades… Hal Varian, 2009. Googles chief economist. In: McKinsey Quarterly 2009 In addition, let’s foster people’s ability to replicate and validate the data work of others. Data literacy is the ability to read, understand, create, and communicate data as information. Colored paragraphs give you a visual overview of things to watch out: A definition is a statement of the meaning of a term. Bazinga highlights a memorable fact. It’s your turn. Do you like to read more about this? "],["about-the-author.html", "About the author", " About the author   Welcome! My name is Marco Kühne. The very first thing I want to do is to invite you to call me Marco. That is, if we meet on the street, you come talk to me during office hours, you ask some question; Marco’s the name that I respond to. Web: http://marco-kuehne.com/ Twitter: https://twitter.com/marco_kuhne GitHub: https://github.com/MarcoKuehne I am a PhD candidate in Economics at the European University Viadrina (https://www.wiwi.europa-uni.de/). I am generally keen on teaching topics related to research design, quantitative methods, and statistical software. My main methodological interests in quantitative social science include panel data modelling, causal inference with observational data and R programming. I am also a gardening fanatic, a coffee enthusiast, an engaged ballroom dancer, a (vegetarian) food lover. I enjoy cutting down big trees and practicing new languages in its own sake. Feel free to contact me! "],["data-is-everywhere.html", "Chapter 1 Data is everywhere 1.1 Why we measure 1.2 How we measure 1.3 Types of data 1.4 The reality of data 1.5 Data sources 1.6 Dataset collections 1.7 Open data", " Chapter 1 Data is everywhere In this chapter you learn: What data is. Why we measure data. How we measure data. What types and kinds of data there are. Selected data sources. 1.1 Why we measure Two stories. 1.1.1 Women are having far fewer children. Figure 1.1: Average number of babies per woman from 1800 to today. Source: Gapminder There is a dramatic change in the number of babies per woman in the last 50 years. The maintenance of a stable human population requires that the mean number of children women should have by the completion of the fertile part of their life is 2.1.1 We cannot know this without measurement. We may have an impression that families are smaller, but that could just be the people we know directly – and most of us know directly at most a couple of hundred households.2 We have to measure to know the big picture and we have to keep measuring to see how the picture is changing. Size matters. Change matters. Without measurement we can describe neither current condition nor the history of current condition. 1.1.2 Global surface temperature is rising. Figure 1.2: Global temperature in the common era. Source: Wikipedia The 2011–2020 decade warmed to an average 1.09 °C [0.95–1.20 °C] compared to the pre-industrial baseline (1850–1900). Figure 1.2 shows a global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue. Directly observed data is in red (Wikipedia contributors 2022). Data is required to make informed decisions. Decisions about climate change are complex, costly and have long-term implications. It is therefore vital that such decisions are based on the best available evidence. We need to understand the quality and provenance of that evidence, and whether any assumptions have been made in generating it. 1.2 How we measure Data collection is the process of gathering and measuring information. As social scientists we rarely count tree rings or analyse corals and ice cores. In social science, common data collection methods are: Surveys (paper vs. online) Interviews Observation Experiment Focus group (qualitative research) Case study (qualitative research) Data can be found at statistical offices. Experiments are mainly used by psychologists. Primary data refers to the first hand data gathered by the researcher or practitioner himself. Secondary data means data collected by someone else. As you expect, it can be more complicated, cost and time expensive to collect data yourself. Data Scraping is the transfer of information from a source like a website to a database. 1.3 Types of data Quantitative data deals with things that are measurable and can be expressed in numbers or figures, or using other values that express quantity. That being said, quantitative data is usually expressed in numerical form and can represent size, length, duration, amount, price, and so on. Qualitative data is often non-numeric. Data can be text, audio or visual. There are quantitative approaches to analyse text (text mining, e.g. sentiment analysis) and visual data (machine learning, e.g. image classification). Sometimes quantitative data is understood as metric continuous as opposed to qualitative data in the form of categorical data. “Qualitative is a systematic, interactive and subjective approach used to describe life experiences and give them meaning” (Burns &amp; Grove, 2006, p. 35). This type of research is conducted to describe and promote understanding of human experience such as stress. While, “Quantitative research is a formal, objective, and systematic process in which numerical data are used to obtain information about the world” (Burns &amp; Grove, 2006, p. 35).3 Cross-sectional data is measured at a point in time (e.g. one year). Longitudinal data is repeated measures over time (e.g. multiple years). Also called panel data. 1.4 The reality of data … it is important not to lose sight of the individuals whose lives provide the data for the models. Although variables rather than individual people may become the subjects of the statistician’s narrative, it is individuals rather than variables who have the capacity to act and reflect on society. Elliott, 1999: 101–2. In: Causal Inference – The Mixtape The Encyclopaedia of Statistical Sciences (1999) describes variables as manifest or observed when they are directly measurable or hidden or latent as “idealized constructs at best only indirectly measurable”. 1.5 Data sources Here is a list of high quality data sources that are useful in social science. German General Social Survey Link: https://www.gesis.org/allbus/allbus Since: 1980 biyearly Access: Register Academic File Formats: Stata, SPSS Data characteristic: cross-sectional Socio-Economic Panel Link: https://www.diw.de/en/soep Since: 1984 yearly Access: Senior researcher as contractor File Formats: Stata, R, SPSS, ASCII Data characteristic: paneldata Notes: Comprehensive online companion http://companion.soep.de/ Research application: Meier, S., &amp; Stutzer, A. (2008). Is volunteering rewarding in itself?. Economica, 75(297), 39-59. Gravity Data by CEPII Link: http://www.cepii.fr/CEPII/en/bdd_modele/bdd_modele_item.asp?id=8 Since: Irregularly, 2015 (full/light), 2020, 2021 File Formats: Stata, R, CSV Access: Register, anyone Versions: Gapminder Story: Gapminder’s stated mission is “Fighting devastating ignorance with fact-based worldviews everyone can understand.” Link: https://www.gapminder.org/data/ File formats: CSV or XLSX Access: Free for everyone Challenge your world view: https://upgrader.gapminder.org/ SlaveVoyages Story: The SlaveVoyages website is a collaborative digital initiative that compiles and makes publicly accessible records of the largest slave trades in history. Link: https://www.slavevoyages.org/ File formats: Excel, CSV Access: Free for everyone Twitter Story: Link: https://developer.twitter.com/ File formats: JSON Access: Register developer account (research track), API WhatsApp File Format: txt Access: You 1.6 Dataset collections Google Dataset Search Link: https://datasetsearch.research.google.com/ Datasets: 25.000.000 Established: Kaggle Link: https://www.kaggle.com/datasets Datasets: About 50.000 Established: 2010 Datahub Link: https://datahub.io/collections R Datasets Link: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html 1.7 Open data https://www.destatis.de/DE/Service/OpenData/_inhalt.html#sprg251244 https://www-genesis.destatis.de/genesis/online free no register References It is 2.1 rather than 2 to allow for the failure of some female children themselves to live through the fertile years of adult life.↩︎ According to Tian Zheng (Columbia College), the average American knows about 600 people. NY times https://www.nytimes.com/2013/02/19/science/the-average-american-knows-how-many-people.html↩︎ https://nursinganswers.net/essays/philosophy-of-quantitative-and-qualitative-nursing-essay.php↩︎ "],["software-is-everywhere.html", "Chapter 2 Software is everywhere", " Chapter 2 Software is everywhere Test it (Meier and Stutzer 2008) References "],["group-comparison.html", "Chapter 3 Group Comparison 3.1 Story Telling 3.2 Comparability 3.3 Never use pie charts 3.4 Application", " Chapter 3 Group Comparison ** Learning Goals** - Conceptual Skills: What makes a good comparison. Never use pie charts. - Data Skills: + R Basics: Install and load a package. Load Stata files into R. + Data Manipulation: tidyverse language (glimpse(), count(), filter(), summarise()). + Data Visualization: Pie chart and bar plot. Social sciences is about the study of human beings and their interactions. As such, we frequently want to compare two or more groups of human beings, organizations, teams, countries, etc., with each other to see whether they are similar or different from each other. Sometimes we also want to track individuals over time and see how they may have changed in some way or other. In short, comparing groups is an essential technique to make inferences and helps us better understand the diversity surrounding us. One big goal of group comparisons is to answer the question: Did it work? Comparing groups can help answer this question. What would have happened if children and families would not have received a program or service? Another big goal of group comparisons is to answer the question: Is there a systematic difference? Groups are hardly ever identical. Comparing groups will reveal differences between them. When group differences are remarkable they might indicate a case of discrimination (think about pay or health). Here is how the The Administration for Children and Families (ACF) explains how and why meaningful comparison groups are important. 3.1 Story Telling Groups can be compared via numeric figures or graphical instruments or a combination of both. Numeric summaries can be the mean, median or mode. But it can be as simple as a percentage or proportion. In summer 2022 5.131 students were enrolled at Viadrina European University. 71 % had a German citizenship and 29 % were international students. A graphical representation can be a boxplot, histogram or density curve. There are more artistic way as well. Note that certain calculations and visualizations only make sense for continuous variables (cf. scale of measurements). Most data visualization is done for the purpose of communication. We have an insight about a dataset, and we have a potential audience, and we would like to convey our insight to our audience. To communicate our insight successfully, we will have to present the audience with a clear and exciting story. Figure 3.1: Possible gender difference in working parttime. The illustration conveys the information: 41 % of women work part-time, whereas only 12 % of men do. The data-ink ratio is the proportion of Ink that is used to present actual data compared to the total amount of ink (or pixels) used in the entire display. Good graphics should include only data-Ink. Non-Data-Ink is to be deleted everywhere where possible. Read more: http://socialmediaguerilla.com/content-marketing/less-is-more-improving-the-data-ink-ratio/ 3.2 Comparability What makes a good comparison? Before we can jump into group comparisons, we need to make ourselves aware of whether our groups can be compared in the first place. We are looking at the characteristics of our groups. Some commonly considered features include: Size: Are the groups about equally large? Time: Was the data collected around the same time? Exogenous variables: Is the distribution of characteristics we are not interested in approximately the same across groups? Don’t be satisfied with the information that is presented to you. Search for context information. 3.3 Never use pie charts Pie charts are evil. Why? Check the note of the documentation of pie(): ?pie() Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data. Cleveland (1985), page 264: “Data that can be shown by pie charts always can be shown by a dot chart. This means that judgements of position along a common scale can be made instead of the less accurate angle judgements.” This statement is based on the empirical investigations of Cleveland and McGill as well as investigations by perceptual psychologists. Can you tell whether Singapore or London have the higher share? Can you tell now? # Plot the chart. barplot(x, names.arg = labels) Here is the R code. # Create data for the graph. x &lt;- c(14, 62, 17, 53) labels &lt;- c(&quot;London&quot;, &quot;New York&quot;, &quot;Singapore&quot;, &quot;Mumbai&quot;) # Plot the chart. pie(x,labels) # Plot the chart. barplot(x, names.arg = labels) 3.4 Application The SOEP practice dataset is highly simplified. It simulates the pattern of real SOEP research data. It comes in Stata format (.dta) and can be downloaded from the homepage of DIW Berlin (in German or English): https://www.diw.de/en/diw_01.c.603154.en/soep_in_the_college_classroom.html There are two versions (years 2000-2004, 9 variables and years 2015-2019, 15 variables). They contain multiple observations per person over time. Each row is uniquely identified by a year-id combination (long format panel data). 3.4.1 Loading the data A .dta file is loaded from GitHub repo with the haven package. Alternatively, download the files and specify a path. # load package library(haven) # read dta file from github soep &lt;- read_dta(&quot;https://github.com/MarcoKuehne/marcokuehne.github.io/blob/main/data/soep_lebensz_en.dta?raw=true&quot;) 3.4.2 Look into data The SOEP practice dataset consists of a total of 9 original variables and 12,922 measurements which we can see with the glimpse() function from the tidyverse package. id is a person identifier. The variables have speaking labels (e.g. year, sex, education). There are two versions of health and satisfaction, one is original and one is standardized. library(tidyverse) glimpse(soep) #&gt; Rows: 12,922 #&gt; Columns: 9 #&gt; $ id &lt;dbl&gt; 312, 399, 399, 457, 457, 457, 748, 761, 761, 1044, 1044, 10… #&gt; $ year &lt;dbl&gt; 2004, 2000, 2001, 2000, 2002, 2004, 2000, 2000, 2001, 2000,… #&gt; $ sex &lt;dbl+lbl&gt; 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1… #&gt; $ education &lt;dbl&gt; NA, 12.0, 12.0, 18.0, 18.0, 18.0, 14.0, 16.0, 16.0, 14.0, 1… #&gt; $ no_kids &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2,… #&gt; $ health_org &lt;dbl+lbl&gt; 4, 4, 4, 3, 3, 2, 3, 3, 3, 4, 3, 4, 4, 4, 2, 2, 2, 1, 2… #&gt; $ satisf_org &lt;dbl+lbl&gt; 7, 8, 9, 8, 7, 4, 5, 7, 7, 5, 7, 8, 6, 7,… #&gt; $ health_std &lt;dbl&gt; 0.5670103, 0.5670103, 0.5670103, -0.4639175, -0.4639175, -1… #&gt; $ satisf_std &lt;dbl&gt; -0.09090909, 0.47727272, 1.04545450, 0.47727272, -0.0909090… glimpse() offers a view in the data, i.e. it shows particular measurements of variables for some units. A similar command is head() which better represents the tabular structure of the data, i.e. rows and columns. Since the data is labelled from Stata, head() also shows these labels. head(soep) #&gt; # A tibble: 6 × 9 #&gt; id year sex education no_kids health_org satisf_org health_std #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 312 2004 1 [female] NA 1 4 [good] 7 [7 Sati… 0.567 #&gt; 2 399 2000 0 [male] 12 1 4 [good] 8 [8 Sati… 0.567 #&gt; 3 399 2001 0 [male] 12 1 4 [good] 9 [9 Sati… 0.567 #&gt; 4 457 2000 0 [male] 18 0 3 [satisfactor… 8 [8 Sati… -0.464 #&gt; 5 457 2002 0 [male] 18 0 3 [satisfactor… 7 [7 Sati… -0.464 #&gt; 6 457 2004 0 [male] 18 0 2 [poor] 4 [4 Sati… -1.49 #&gt; # … with 1 more variable: satisf_std &lt;dbl&gt; Sine the coding is 1 for female and 0 for male it is a valid alternative to rename() the variable into a female binary dummy. The logic of the code is this: take soep data and do the following (e.g. %&gt;% pipe operator), rename old variable sex as female. Store the result again in soep, i.e. overwrite the old data. soep &lt;- soep %&gt;% rename(female = sex) It it likely that not all possible labels appear in the first 6 rows. Access labels for categorical data with the attribute function attr(). attr(soep$health_org, &quot;labels&quot;) #&gt; not valid does not concern no answer bad #&gt; -3 -2 -1 1 #&gt; poor satisfactory good very good #&gt; 2 3 4 5 3.4.3 Fly over data Now let’s have a look at summary statistics for the dataset. Summary statistics or Descriptive Statistics often comprise measures of location and spread. The base R function summary() can be applied to the entire soep data. summary() produces results for all variables. The data scientist has to decide whether this makes sense. It is not useful to calculate the mean or median of id. In the following, we will ignore the first two variables and select column 3 to 9 by soep[,(3:9)]. summary(soep) #&gt; id year female education #&gt; Min. : 312 Min. :2000 Min. :0.0000 Min. : 7.00 #&gt; 1st Qu.:166206 1st Qu.:2001 1st Qu.:0.0000 1st Qu.:10.50 #&gt; Median :333851 Median :2002 Median :1.0000 Median :11.50 #&gt; Mean :335619 Mean :2002 Mean :0.5389 Mean :11.87 #&gt; 3rd Qu.:502770 3rd Qu.:2003 3rd Qu.:1.0000 3rd Qu.:13.00 #&gt; Max. :677912 Max. :2004 Max. :1.0000 Max. :18.00 #&gt; NA&#39;s :1468 #&gt; no_kids health_org satisf_org health_std #&gt; Min. :0.0000 Min. :1.000 Min. : 0.000 Min. :-2.525773 #&gt; 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.: 6.000 1st Qu.:-0.463918 #&gt; Median :0.0000 Median :4.000 Median : 7.000 Median : 0.567010 #&gt; Mean :0.6466 Mean :3.455 Mean : 7.156 Mean : 0.005419 #&gt; 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.: 8.000 3rd Qu.: 0.567010 #&gt; Max. :3.0000 Max. :5.000 Max. :10.000 Max. : 1.597938 #&gt; NA&#39;s :906 NA&#39;s :15 NA&#39;s :15 #&gt; satisf_std #&gt; Min. :-4.068182 #&gt; 1st Qu.:-0.659091 #&gt; Median :-0.090909 #&gt; Mean :-0.002265 #&gt; 3rd Qu.: 0.477273 #&gt; Max. : 1.613636 #&gt; The stargazer package (with the option type=\"text\") displays the number of observations, mean, standard deviation, min, max and percentiles to the console or markdown file (it produces a .tex table by default). stargazer requires a dataframe as input. haven created a tibble from the Stata .dta file. Finally, we select all variables but the first two, since they are identifier and any calculation, e.g. the mean of personal IDs, does not make much sense here. library(stargazer) stargazer(as.data.frame(soep)[,(3:9)], type=&quot;text&quot;) #&gt; #&gt; =============================================== #&gt; Statistic N Mean St. Dev. Min Max #&gt; ----------------------------------------------- #&gt; female 12,922 0.539 0.499 0 1 #&gt; education 11,454 11.874 2.469 7.000 18.000 #&gt; no_kids 12,016 0.647 0.931 0 3 #&gt; health_org 12,907 3.455 0.973 1 5 #&gt; satisf_org 12,922 7.156 1.762 0 10 #&gt; health_std 12,907 0.005 1.003 -2.526 1.598 #&gt; satisf_std 12,922 -0.002 1.001 -4.068 1.614 #&gt; ----------------------------------------------- A more modern approach comes from the modelsummary package. The datasummary_skim function tells us the unique number of values, a percentage of missing values, mean, standard deviation, minimum, median, maximum and show a tiny picture of the distribution of the variable. library(modelsummary) datasummary_skim(soep[,(3:9)]) Unique (#) Missing (%) Mean SD Min Median Max female 2 0 0.5 0.5 0.0 1.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } education 14 11 11.9 2.5 7.0 11.5 18.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } no_kids 5 7 0.6 0.9 0.0 0.0 3.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } health_org 6 0 3.5 1.0 1.0 4.0 5.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } satisf_org 11 0 7.2 1.8 0.0 7.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } health_std 6 0 0.0 1.0 −2.5 0.6 1.6 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } satisf_std 11 0 −0.0 1.0 −4.1 −0.1 1.6 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Think about The mean of female is 0.539. What does it tell you? 3.4.4 Missing data datasummary_skim tells that 11 % of education is missing. Missing information is represented by NA in R. Check out again the first call of glimpse() that shows the first entry of education which is missing. NA stand for non available and is the indicator for missing data in R. We ask the table function to count of all values in the data that are either missing or not missing (is.na()). table(is.na(soep)) #&gt; #&gt; FALSE TRUE #&gt; 113894 2404 There are options to deal with missing data. One common approach is substituting values for missing cells, e.g. the mean (imputation). The alternative, for simplicity, is removing all rows which contain a missing value in any column. This is called a complete case analysis. # It is your decision to overwrite soep after a data operation or to create a new data object. soep_no_na &lt;- soep %&gt;% filter(complete.cases(.)) In the following, soep_no_na data is used. 3.4.5 Explore data The count() function provides the number of occurrences for values of a variables. soep_no_na %&gt;% count(year) #&gt; # A tibble: 5 × 2 #&gt; year n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2000 3000 #&gt; 2 2001 2248 #&gt; 3 2002 1965 #&gt; 4 2003 1763 #&gt; 5 2004 1683 We observe a decline in observations over time. People drop out of the survey for various reasons. SOEP regularly adds refreshment samples to compensate for this. Sometimes we miss data in between years. panel attrition Person number 457 participated in 2000, 2002 and 2004: soep_no_na %&gt;% filter(id == 457) #&gt; # A tibble: 3 × 9 #&gt; id year female education no_kids health_org satisf_org health_std #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 457 2000 0 [male] 18 0 3 [satisfactory] 8 [8 Sati… -0.464 #&gt; 2 457 2002 0 [male] 18 0 3 [satisfactory] 7 [7 Sati… -0.464 #&gt; 3 457 2004 0 [male] 18 0 2 [poor] 4 [4 Sati… -1.49 #&gt; # … with 1 more variable: satisf_std &lt;dbl&gt; Person 457 was not available in the year 2001 for some reason. Attention with real research data, we never show personal ID. arrange() orders the rows of data by the values of selected columns (from low to high). We can combine arrange() with desc() to order observations from high to low. soep_no_na %&gt;% arrange(desc(no_kids)) #&gt; # A tibble: 10,659 × 9 #&gt; id year female education no_kids health_org satisf_org health_std #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 2878 2000 1 [female] 7 3 4 [good] 7 [7 Sati… 0.567 #&gt; 2 5728 2003 1 [female] 11.5 3 3 [satisfacto… 8 [8 Sati… -0.464 #&gt; 3 5728 2004 1 [female] 11.5 3 3 [satisfacto… 5 [5 Sati… -0.464 #&gt; 4 8741 2000 1 [female] 11 3 4 [good] 5 [5 Sati… 0.567 #&gt; 5 8741 2002 1 [female] 10.5 3 2 [poor] 6 [6 Sati… -1.49 #&gt; 6 8741 2003 1 [female] 10.5 3 4 [good] 4 [4 Sati… 0.567 #&gt; 7 8741 2004 1 [female] 10.5 3 3 [satisfacto… 8 [8 Sati… -0.464 #&gt; 8 10566 2000 0 [male] 10.5 3 5 [very good] 8 [8 Sati… 1.60 #&gt; 9 10566 2001 0 [male] 10.5 3 5 [very good] 8 [8 Sati… 1.60 #&gt; 10 12599 2000 0 [male] 9 3 4 [good] 8 [8 Sati… 0.567 #&gt; # … with 10,649 more rows, and 1 more variable: satisf_std &lt;dbl&gt; soep_no_na %&gt;% group_by(female) %&gt;% summarise(median(education)) #&gt; # A tibble: 2 × 2 #&gt; female `median(education)` #&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 0 [male] 11.5 #&gt; 2 1 [female] 11.5 3.4.6 Standardization Look again at the glimpse() output or stargazer() table. Notice two versions of satisfaction and health, i.e. satisf_org and satisf_std which represent satisfaction on the original scale (from 0 to 10) and a standardized version of satisfaction. Health is ranging from 1 to 5. How can we compare values of satisfaction and health when their range is different? In statistics, standardization is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables (compare apples with apples instead of apples with bananas). Typically, to standardize variables, you calculate the mean and standard deviation for a variable (cf. z-score). Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. Consequently, standardized variables have a mean of 0 and a standard deviation of 1. soep_no_na %&gt;% summarise(across(c(satisf_std, health_std), list(mean, sd))) %&gt;% `colnames&lt;-`(c(&quot;mean of std. satis&quot;, &quot;sd of std. satis&quot;, &quot;mean of std. health&quot;, &quot;sd of std. health&quot;)) #&gt; # A tibble: 1 × 4 #&gt; `mean of std. satis` `sd of std. satis` `mean of std. health` `sd of std. he…` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00174 1.00 -0.0102 0.998 Here summarise is asked to apply the mean() and sd() function “across” the two columns satisf_std and health_std. The means are close to zero and the standard deviation is close to one. The minor differences result from our complete case analysis (we already dropped some cases which might have been used for the initial standardization). "],["mean-comparison.html", "Chapter 4 Mean Comparison 4.1 Learning Goals 4.2 Student’s t-test 4.3 Plot with Confidence 4.4 Independent t-test 4.5 Dependent t-test", " Chapter 4 Mean Comparison 4.1 Learning Goals Conceptual Skills: Compare means within and between people. What Student’s t-test has to do with beer. Data Concepts: Factor variables. Data Skills: Data Manipulation: Several options to achieve a goal. Data Analysis: Hypothesis testing with t-Tests. Data Visualization: Error plots. The term mean comparisons refers to the comparison of the average of one or more continuous variables over one or more categorical variables. It is a general term that can refer to a large number of different research questions and study designs. For example, one can compare the mean from one sample of data to a hypothetical population value, compare the means on a single variable from multiple independent groups, or compare the means for a single variable for one sample over multiple measurement occasions. In addition, more complex research designs can employ multiple continuous dependent variables simultaneously, as well as a combination of multiple groups and multiple measurement occasions. Overall, mean comparisons are of central interest in any experimental design and many correlational designs. 4.1.1 Data Preparation Load SOEP practice data from Github repository directly into memory. It makes data loading independent of paths. library(haven) soep &lt;- read_dta(&quot;https://github.com/MarcoKuehne/seminars_in_applied_economics/blob/main/Data/soep_lebensz_en.dta?raw=true&quot;) ## Sample (complete case study) library(tidyverse) soep_no_na &lt;- soep %&gt;% filter(complete.cases(.)) ## subset the year 2000 soep_2000 &lt;- subset(soep_no_na, year == 2000) ## Fixing factors soep_2000$sex_factor &lt;- as.factor(soep_2000$sex) levels(soep_2000$sex_factor) &lt;- c(&quot;male&quot;, &quot;female&quot;) ## Convert Stata format library(sjlabelled) soep_2000 &lt;- remove_all_labels(soep_2000) 4.1.2 Grouped Mean There are several options to calculate means per group. ## TAPPLY means1 &lt;- tapply(soep_2000$health_org, INDEX=soep_2000$sex, FUN=mean) ## AGGREGATE means2 &lt;- aggregate(health_org~sex, soep_2000, mean) ## BY means3 &lt;- by(soep_2000$health_org, soep_2000$sex, mean) ## DPLYR pipe library(tidyverse) means4 &lt;- soep_2000 %&gt;% group_by(sex)%&gt;% summarize(means = mean(health_org)) means_m &lt;- soep_2000 %&gt;% filter(sex==0) %&gt;% summarize(mean_education=mean(health_org)) means_f &lt;- soep_2000 %&gt;% filter(sex==1) %&gt;% summarize(mean_education=mean(health_org)) Check how different means1, means2 and means3 look like in the console. means1 #&gt; 0 1 #&gt; 3.509326 3.462017 4.1.3 Furious with Factors In order to make labels nice and usable, you need to define sex as a factor variable. A factor variable is a variable used for categorical data. Some categories are ordered (e.g. small, medium, large), some are not (e.g. men, women). An integer or string can be converted to a factor. Factors have hidden levels. Several R functions and packages treat factors fundamentally different from integers. When R handles sex as an integer, it assumes that values such as sex=-3 and sex=1.5 are possible. This is what happens to numbers. gender_numbers &lt;- c(0, 1, 0, 1, 1) summary(gender_numbers) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0 0.0 1.0 0.6 1.0 1.0 table(gender_numbers) #&gt; gender_numbers #&gt; 0 1 #&gt; 2 3 A mean can be calculated. Compare to character coding. gender_words &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;) summary(gender_words) #&gt; Length Class Mode #&gt; 5 character character table(gender_words) #&gt; gender_words #&gt; female male #&gt; 3 2 Now, convert the gender variable to a factor and calculate the mean again. soep_2000$sex_factor &lt;- as.factor(soep_2000$sex) levels(soep_2000$sex_factor) &lt;- c(&quot;male&quot;, &quot;female&quot;) means &lt;- tapply(soep_2000$health_org, INDEX=soep_2000$sex_factor, FUN=mean) means #&gt; male female #&gt; 3.509326 3.462017 The average health status is higher for men in comparison to women in the data. What else can we say about this difference? Is it small or large? A cause for concern? How can we become more confident of the result? We do a statistical test that compares the means. Hypothesis testing in statistics is a way to test the results of a survey or experiment to see if there are meaningful results. It’s basically testing whether the results are valid by figuring out the odds that the results have happened by chance. If the results may have happened by chance, the experiment won’t be repeatable and so has little use. 4.2 Student’s t-test What this has do to with beer. Student’s t-test or simply t-test was published W.S. Gossett who hid his name due to his position as a worker in a brewery company (Guiness Brewery Dublin). Here is a t-test (it takes a formula as input, the relation between health_org and sex): t.test(health_org ~ sex, data=soep_2000) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: health_org by sex #&gt; t = 1.3118, df = 2968.8, p-value = 0.1897 #&gt; alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.02340655 0.11802305 #&gt; sample estimates: #&gt; mean in group 0 mean in group 1 #&gt; 3.509326 3.462017 Look at the output of the t-test: The so called t-statistic is 1.3117519 The degrees of freedom are 2968.8493968 The p-value of 0.1897053 The 95% confidence interval is [-0.0234066;0.118023] The t-statistic is at the heart of the hypothesis test. The t value has to be compared to a critical value in a t table to decide whether or not to reject the null hypothesis (H0). The null states: Move along, there’s nothing to see here! The p-value can be seen as easy-alternative to the t-statistic. You probably never look up critical t values, do you? If you just want to glimpse at it, here is one. We are looking forward getting small p-values (smaller than 0.05 by convention). If p &lt; 0.05 we say that we reject the null and conclude that there is actually something going on! If a p value is less than 0.05, the result is called statistically significant. The 95% confidence interval (CI) is another way of looking at the same issue. 95% of the values should be in the interval 0.1 to 0.3. Thus if zero is not in the CI we can be pretty sure that something is going on. Apparently, degrees of freedom prefer to keep an air of mystery about them. Actually there a several different versions of the t.test and t.test() tells you that it’s a Welch Two Sample t-test. Unfortunately, Bernard Lewis Welch did not work for a Brewery company. The Welch’s t-test (or unequal variances t-test), is a two-sample location test which is used to test the hypothesis that two populations have equal means. 4.3 Plot with Confidence Time to plot. In addition to the group means we plot confidence intervals using the gplots package with the plotmeans() function. The result is this: plotmeans() nicely presents the number of observations and labels by default. There is a statistically significant difference, when confidence intervals not overlap. Conclusion: Don’t worry about the health differential. 4.4 Independent t-test Now we learn about two different versions of the two sample t-test: Independent (or unpaired) t-test Dependent (or paired or repeated) t-test There is a one sample t-test which does not help much for group comparison. You can read more about Unpaired and paired two-sample t-tests. The first t.test on health between gender was unpaired since the default option is paired=FALSE. We used the unpaired t-test for gender, since men and women are independent in the sense of statistical samples (though this may be different in other relationships). For the next calculation we create another subsample and focus on all women who are observed in 2000 and 2004. We can compare women to men. We can compare women to themselves. women_before_after &lt;- soep_no_na %&gt;% filter(sex == 1) %&gt;% ## filter all males filter(year == 2000 | year == 2004) %&gt;% ## only use year 2000 or 2004 group_by(id) %&gt;% ## look at id groups filter(n() == 2) ## only keep groups with exactly two observations/rows The unpaired t-test on women’s health is: t.test(health_org ~ year, data=women_before_after, paired=FALSE) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: health_org by year #&gt; t = 3.6158, df = 1629.9, p-value = 0.0003085 #&gt; alternative hypothesis: true difference in means between group 2000 and group 2004 is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.07850093 0.26463633 #&gt; sample estimates: #&gt; mean in group 2000 mean in group 2004 #&gt; 3.497549 3.325980 It looks like women have been significantly more healthy in 2000 than in 2004. Can you think of a reason? What happened to all those women? 4.5 Dependent t-test Now run the t.test as paired=TRUE. t.test(health_org ~ year, data=women_before_after, paired=TRUE) #&gt; #&gt; Paired t-test #&gt; #&gt; data: health_org by year #&gt; t = 5.7026, df = 815, p-value = 1.65e-08 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.1125131 0.2306242 #&gt; sample estimates: #&gt; mean difference #&gt; 0.1715686 The paired t-test is a little bit more exiting in the way that we need data on something observed at two different points in time. In this before-and-after scenario, the person serves as something like its own control group. We’ll come back to that. The mean difference is exactly the same (can you see it? - ask your console to calculate the difference between 3.497549 and 3.325980). But all the other statistics seem to be different: t-value, degree of freedom, p-value … In this scenario the mean difference from the paired=TRUE and paired=FALSE test are identical. But using the information from the paired sample (one person observed for two periods), the t-value became higher and thus the p-value lower. In a nutshell, we gained more statistical confidence in our estimate! Once more we saw how valuable time is. We explore this in more detail in the next chapter. Now, you can call yourself … "],["panel-data.html", "Chapter 5 Panel Data 5.1 Learning Goals 5.2 Paneldata 5.3 Data Preparation 5.4 Group by person 5.5 Data Visualization 5.6 Within and Between Variation", " Chapter 5 Panel Data 5.1 Learning Goals Conceptual Skills: Understand panel data. Panel attrition. Distinguish between and within variation. Compare the paired and unpaired t-Test to panel data. Data Skills: How set.seed() defeats randomness. Find and use functions from other people. Tidyverse e.g. group_by(), sample_n(), summarize(), count(), arrange(). Data Visualization: Panel plots (time series per group). 5.2 Paneldata Panel data are observations for the same subjects over time. Subjects can be people, households, firms or countries. Panel data are a subset of longitudinal data. Key components are the panel identifier: person (id) and time (year). Every row is a person-year combination (so called long format). Compare these types of data: Cross-section: One time measurement. Repeated cross-section: Different people. Panel: Repeated observations of the same individuals. Figure 5.1: Table 1: Types of Data. Panel data allows for special statistical analyses, e.g. fixed effects regression. With panel data we know the time-ordering of events. Panel data allow to identify causal effects under weaker assumptions (compared to cross-sectional data). Different types of data can answer different types of questions: Levels: How many people are poor in 2016 in Germany? Trends: Has poverty increased or decreased? Change: How many people went in and out of poverty? The unemployment rate in Germany is about 5 % over the last few years (Source: Annual average unemployment rate in Germany from 2005 to 2022). Imagine a college graduate enters the job market. It takes about 3 month to search for jobs, send applications and do the job interviews. Finally she finds a perfect match. After some years the college graduate falls in love with someone who lives in another city, so the student moves to this city, again looking for a job at a new employer. Imagine another college graduate. He sends 5 applications for various jobs per week. He has several mini jobs to keep afloat. The student realizes that he did not acquire the skills that are expected on the job market. After 2 years, he finally finds a suitable position. Do you think that is a cause for concern? Life after collage. alt text Panel data allows to analyze the level of unemployment in Germany as well as the changes and trajectories of individuals. We can separate a frictional unemployment component and a permanent unemployment share. Frictional unemployment is a form of unemployment reflecting the gap between someone voluntarily leaving a job and finding another. As such, it is sometimes called search unemployment. Do we accept search unemployment? Is it different from long-term unemployment? What do you think. Some famous household panel data studies are: United States: Panel Study of Income Dynamics (PSID) since 1968 Germany: Socio-Economic Panel (SOEP) since 1984 United Kingdom: British Household Panel Survey (BHPS) since 1991 Australia: Household, Income and Labour Dynamics in Australia Survey (HILDA) since 2001 These scientific datasets can often be analyzed for research and student theses free of charge. 5.3 Data Preparation Load SOEP practice data from web into temporal memory and into R. library(haven) soep &lt;- read_dta(&quot;https://github.com/MarcoKuehne/seminars_in_applied_economics/blob/main/Data/soep_lebensz_en.dta?raw=true&quot;) 5.4 Group by person group_by() is a powerful command, especially when working with panel data. Use count() after group_by() to get the number of observations of each person (id) in the data: library(tidyverse) head(soep %&gt;% group_by(id) %&gt;% count()) See result. #&gt; # A tibble: 6 × 2 #&gt; # Groups: id [6] #&gt; id n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 312 1 #&gt; 2 399 2 #&gt; 3 457 3 #&gt; 4 748 1 #&gt; 5 761 2 #&gt; 6 1044 5 Adding arrange() means sorting the data by a variable (i.e. the n) either ascending or descending (from high to low). soep %&gt;% group_by(id) %&gt;% count() %&gt;% arrange(desc(n)) See result. #&gt; # A tibble: 3,550 × 2 #&gt; # Groups: id [3,550] #&gt; id n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1044 5 #&gt; 2 1300 5 #&gt; 3 1343 5 #&gt; 4 1889 5 #&gt; 5 2160 5 #&gt; 6 2241 5 #&gt; 7 3240 5 #&gt; 8 3535 5 #&gt; 9 3787 5 #&gt; 10 4107 5 #&gt; # … with 3,540 more rows Note that the output further tells you # A tibble: 3,550 x 2, i.e. there are 3550 ID-groups (or units or people). Remember that the observation period is between 2000 and 2004, i.e. min is 1 year, max is 5 years. #&gt; #&gt; 2000 2001 2002 2003 2004 #&gt; 3198 2690 2485 2299 2250 Attrition is the process of dropout from a panel study. How many people are observed in all years? Filter for a specific n and check the tibble size again. soep %&gt;% group_by(id) %&gt;% count() %&gt;% filter(n == 5) See result. #&gt; # A tibble: 1,851 × 2 #&gt; # Groups: id [1,851] #&gt; id n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1044 5 #&gt; 2 1300 5 #&gt; 3 1343 5 #&gt; 4 1889 5 #&gt; 5 2160 5 #&gt; 6 2241 5 #&gt; 7 3240 5 #&gt; 8 3535 5 #&gt; 9 3787 5 #&gt; 10 4107 5 #&gt; # … with 1,841 more rows 5.5 Data Visualization 5.5.1 Average Health per Year A picture is worth a thousand words. Show a time trend or a relationship at the beginning of your analysis. People fancy plots. Your plot should be interesting and easy to understand. If you think obesity or crime is a problem for society and it’s increasing over time you made a good point of why we should talk about it or analyze it more thoroughly. Actually, after months and years of empirical research, the results often can be summarized in one single plot. Health status in Germany is stable. 5.5.2 Individual Health per Year Next, we dig deeper into personal health stories. 5.5.2.1 Random Panel Sample Let’s use sample_n() for random selection of observations. Each time this command is run, a different random set of observations is drawn. Note that these 5 random observations/rows do not stem from the same people. soep %&gt;% sample_n(5) #&gt; # A tibble: 5 × 9 #&gt; id year sex education no_kids health_org satisf_org health_std #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 103076 2002 0 [male] 11.5 1 2 [poor] 7 [7 Sati… -1.49 #&gt; 2 32327 2000 1 [female] 9 0 4 [good] 7 [7 Sati… 0.567 #&gt; 3 102539 2001 1 [female] NA 1 3 [satisfacto… 5 [5 Sati… -0.464 #&gt; 4 487511 2003 0 [male] 14 0 4 [good] 8 [8 Sati… 0.567 #&gt; 5 303234 2000 1 [female] 9 0 2 [poor] 5 [5 Sati… -1.49 #&gt; # … with 1 more variable: satisf_std &lt;dbl&gt; If you plant a seed with set.seed randomness becomes reproducible. set.seed(42) soep %&gt;% sample_n(5) #&gt; # A tibble: 5 × 9 #&gt; id year sex education no_kids health_org satisf_org health_std #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 564721 2001 1 [female] 11.5 0 4 [good] 8 [8 Sati… 0.567 #&gt; 2 643474 2003 0 [male] NA 2 4 [good] 7 [7 Sati… 0.567 #&gt; 3 121058 2003 1 [female] 10.5 0 3 [satisfacto… 5 [5 Sati… -0.464 #&gt; 4 268036 2004 1 [female] NA 0 3 [satisfacto… 9 [9 Sati… -0.464 #&gt; 5 481638 2004 0 [male] 11.5 2 4 [good] 8 [8 Sati… 0.567 #&gt; # … with 1 more variable: satisf_std &lt;dbl&gt; 5.5.2.2 Personalize Functions Is there a sample_n_groups command in the tidyverse? Unfortunately not. Even in such a case, it’s unlikely that you need to invent something new. One of the first Google hits is: https://cmdlinetips.com/2019/07/how-to-randomly-select-groups-in-r-with-dplyr/ and they suggest: sample_n_groups = function(grouped_df, size, replace = FALSE, weight=NULL) { grp_var &lt;- grouped_df %&gt;% groups %&gt;% unlist %&gt;% as.character random_grp &lt;- grouped_df %&gt;% summarise() %&gt;% sample_n(size, replace, weight) %&gt;% mutate(unique_id = 1:NROW(.)) grouped_df %&gt;% right_join(random_grp, by=grp_var) %&gt;% group_by_(grp_var) } If you run this chunk the function sample_n_groups appears in the right upper panel in RStudio. Now you can use this particular function in your current session. Don’t worry, you don’t have to understand everything from the code provided by others, but make sure that it behaves the way you expect it to. Since this function is based on dplyr commands you can use it within a pipe chain, e.g. pipe it into a ggplot. set.seed(42+42) soep %&gt;% group_by(id) %&gt;% sample_n_groups(3) %&gt;% # select all given observations across time for 5 people ggplot(aes(x = year, y = health_org, group=as_factor(id), color=as_factor(id))) + geom_line() + geom_point() + labs(title=&quot;Health Status per Person&quot;, x=&quot;Year&quot;, y=&quot;Health Status&quot;, color=&quot;Person&quot;) + ylim(1,5) This is test data from the practice data set. It might be helpful to understand what people we selected to compare our code results. Working with real questionnaire data, next show personal information. If you really need a graph like this, drop the legend. Never show person IDs in a report. 5.5.2.3 Minimum Observations per Group The full potential of panel data is to track people over time. We can literally follow people from graduation, to their first job, marriage, first child and buying yourself a house. But we hardly see that from the previous plot for all people. We select randomly 3 people who are observed at least 3 times and plot their life satisfaction over time in a line plot (again, there are several options). set.seed(42+42) soep %&gt;% group_by(id) %&gt;% filter(n()&gt;=3) %&gt;% # the moment we drop some data sample_n_groups(3) %&gt;% # select all given observations across time for 3 people ggplot(aes(x = year, y = health_org, col=factor(id))) + geom_line() + geom_point() + # add points connected by lines labs(title=&quot;Health Status over Time by Person&quot;, subtitle=&quot;Random Sample of 3 People with at least 3 observations.&quot;, x=&quot;Year&quot;, y=&quot;Health Status&quot;, colour=&quot;Person&quot;) + # give some illustrative labels ylim(1,5) # limit the scale always to all possible values, to prevent random misperceptions 5.6 Within and Between Variation Here is a similar plot of life satisfaction for selected people. Person 293973 reports a dramatic change in satisfaction from one year to another. Starting from perfect happiness in 2000 (= 10), it drops to complete misery (= 0) and recovers immediately in 2002 (= 10). Is person 293973 an outlier? Can you think of reasonable scenarios behind the data? Job’s news: take away all that a person holds dear, including children, health, and property. Because this is practice data and we cannot dig deeper into life events potentially triggering these changes. Look at the previous graph from this perspective: Within variation: Within means per individual, group or unit over time. Between variation: Between means comparing individuals, groups or units (both at a point in time or over time). In 2000 person 293973 and person 317284 both reported a happiness value of 10. In 2000, there is no difference between those two people. In 2001 though, 293973 experienced a critical shock and dropped to 0 while 317284 stayed at the very same value of 10. In 2001, the difference between both is 10. Compare the years 2000 and 2001 for person 293973. The value changes within that person by -10, whereas there is no change for person 317284. You often read about changes within people (across time) and about differences between them. In the line plot, within changes happens along the horizontal x-axis, whereas between differences can be measured as the vertical distance on the y-axis. Between and within variation are new properties of a panel data variable. To fully exploit panel data, we need enough within variation. Is there enough within variation? Figure 5.2: Figure 2: Screenshot from a lecture script of Brüderl and Ludwig. The software is Stata. 5.6.1 XTSUM (from Stata) In Stata there is a command xtsum which calculates overall, between and within variation. This is not implemented in any R package so far. Still, clever people created another personal function on Stack Overflow https://stackoverflow.com/questions/49282083/xtsum-command-for-r: Show me the crazy xtsum code. # Again, copy the code and run it, and you can use it in your R session. library(rlang) XTSUM &lt;- function(data, varname, unit) { varname &lt;- enquo(varname) loc.unit &lt;- enquo(unit) ores &lt;- data %&gt;% summarise(ovr.mean=mean(!! varname, na.rm=TRUE), ovr.sd=sd(!! varname, na.rm=TRUE), ovr.min = min(!! varname, na.rm=TRUE), ovr.max=max(!! varname, na.rm=TRUE), ovr.N=sum(as.numeric((!is.na(!! varname))))) bmeans &lt;- data %&gt;% group_by(!! loc.unit) %&gt;% summarise(meanx=mean(!! varname, na.rm=T), t.count=sum(as.numeric(!is.na(!! varname)))) bres &lt;- bmeans %&gt;% ungroup() %&gt;% summarise(between.sd = sd(meanx, na.rm=TRUE), between.min = min(meanx, na.rm=TRUE), between.max=max(meanx, na.rm=TRUE), Units=sum(as.numeric(!is.na(t.count))), t.bar=mean(t.count, na.rm=TRUE)) wdat &lt;- data %&gt;% group_by(!! loc.unit) %&gt;% mutate(W.x = scale(!! varname, scale=FALSE)) wres &lt;- wdat %&gt;% ungroup() %&gt;% summarise(within.sd=sd(W.x, na.rm=TRUE), within.min=min(W.x, na.rm=TRUE), within.max=max(W.x, na.rm=TRUE)) return(list(ores=ores,bres=bres,wres=wres)) } We can use this functionality to compare our prior results. library(sjlabelled) minipanel &lt;- soep %&gt;% filter(id %in% c(293973,317284)) %&gt;% remove_all_labels() XTSUM(minipanel, varname=satisf_org, unit=id) #&gt; $ores #&gt; ovr.mean ovr.sd ovr.min ovr.max ovr.N #&gt; 1 8.625 3.50255 0 10 8 #&gt; #&gt; $bres #&gt; # A tibble: 1 × 5 #&gt; between.sd between.min between.max Units t.bar #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2.22 6.67 9.8 2 4 #&gt; #&gt; $wres #&gt; # A tibble: 1 × 3 #&gt; within.sd within.min within.max #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.10 -6.67 3.33 XTSUM tells us some more results. The overall mean is 8.62 as expected. The overall smallest value of satisfaction is 0, the highest is 10. We saw this in the graph. We have N=8 observations or rows. But we only have n=2 units (see the medium panel). Units (people) are observed on average for t.bar = 4 periods (one person 3 years, one person 5 years, makes 4 years on average). Standard deviation sd is at the core of within and between variation (since it is a measure of dispersion or variation). In the previous graph, we focused on within changes and between differences. Think about examples (i.e. variables) that you expect to not have within variation. Run XTSUM() on different variables to test your knowledge. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
